{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Device configuration\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE = torch.device(\"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## data prepocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_data(\n",
        "    ratings_path: str = \"data/ratings_small.csv\",\n",
        "    movies_path: str = \"data/movies_metadata.csv\",\n",
        "    min_user_interactions: int = 5,\n",
        "    min_movie_interactions: int = 5\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[int, int], Dict[int, int], Dict[int, str]]:\n",
        "    ratings = pd.read_csv(ratings_path)\n",
        "    print(f\"Loaded {len(ratings)} ratings\")\n",
        "    \n",
        "    movies = pd.read_csv(movies_path, low_memory=False)\n",
        "    movies = movies[movies['id'].apply(lambda x: str(x).isdigit())]\n",
        "    movies['id'] = movies['id'].astype(int)\n",
        "    print(f\"Loaded {len(movies)} movies\")\n",
        "    \n",
        "    user_counts = ratings['userId'].value_counts()\n",
        "    valid_users = user_counts[user_counts >= min_user_interactions].index\n",
        "    ratings = ratings[ratings['userId'].isin(valid_users)]\n",
        "    print(f\"After user filtering: {len(ratings)} ratings\")\n",
        "    \n",
        "    movie_counts = ratings['movieId'].value_counts()\n",
        "    valid_movies = movie_counts[movie_counts >= min_movie_interactions].index\n",
        "    ratings = ratings[ratings['movieId'].isin(valid_movies)]\n",
        "    print(f\"After movie filtering: {len(ratings)} ratings\")\n",
        "    \n",
        "    unique_users = ratings['userId'].unique()\n",
        "    unique_movies = ratings['movieId'].unique()\n",
        "    \n",
        "    user2idx = {user: idx for idx, user in enumerate(unique_users)}\n",
        "    movie2idx = {movie: idx + 1 for idx, movie in enumerate(unique_movies)}  # 0 - padding\n",
        "    idx2movie = {idx: movie for movie, idx in movie2idx.items()}\n",
        "    \n",
        "    movie_id_to_title = dict(zip(movies['id'], movies['title']))\n",
        "    idx2movie_title = {}\n",
        "    for idx, movie_id in idx2movie.items():\n",
        "        title = movie_id_to_title.get(movie_id, f\"Movie_{movie_id}\")\n",
        "        idx2movie_title[idx] = title\n",
        "    \n",
        "    print(f\"Unique users: {len(unique_users)}\")\n",
        "    print(f\"Unique movies: {len(unique_movies)}\")\n",
        "    \n",
        "    return ratings, movies, user2idx, movie2idx, idx2movie_title\n",
        "\n",
        "\n",
        "def create_user_sequences(\n",
        "    ratings: pd.DataFrame,\n",
        "    user2idx: Dict[int, int],\n",
        "    movie2idx: Dict[int, int],\n",
        "    min_rating: float = 3.5\n",
        ") -> Dict[int, List[int]]:\n",
        "    print(f\"Creating sequences (min_rating={min_rating})...\")\n",
        "    \n",
        "    positive_ratings = ratings[ratings['rating'] >= min_rating].copy()\n",
        "    print(f\"Positive interactions: {len(positive_ratings)}\")\n",
        "    \n",
        "    positive_ratings = positive_ratings.sort_values(['userId', 'timestamp'])\n",
        "    \n",
        "    user_sequences = defaultdict(list)\n",
        "    for _, row in tqdm(positive_ratings.iterrows(), total=len(positive_ratings), desc=\"Creating sequences\"):\n",
        "        user_idx = user2idx[row['userId']]\n",
        "        movie_idx = movie2idx[row['movieId']]\n",
        "        user_sequences[user_idx].append(movie_idx)\n",
        "    \n",
        "    user_sequences = {k: v for k, v in user_sequences.items() if len(v) >= 3}\n",
        "    \n",
        "    print(f\"Users with sequences >= 3: {len(user_sequences)}\")\n",
        "    \n",
        "    seq_lengths = [len(v) for v in user_sequences.values()]\n",
        "    print(f\"Sequence length statistics:\")\n",
        "    print(f\"  Min: {min(seq_lengths)}, Max: {max(seq_lengths)}, Mean: {np.mean(seq_lengths):.2f}\")\n",
        "    \n",
        "    return dict(user_sequences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## dataset & dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MovieSequenceDataset(Dataset):\n",
        "   \n",
        "    def __init__(\n",
        "        self,\n",
        "        user_sequences: Dict[int, List[int]],\n",
        "        max_seq_len: int = 50,\n",
        "        num_items: int = None\n",
        "    ):\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_items = num_items\n",
        "        self.samples = []\n",
        "        \n",
        "        for user_idx, sequence in user_sequences.items():\n",
        "            for i in range(1, len(sequence)):\n",
        "                input_seq = sequence[max(0, i - max_seq_len):i]\n",
        "                target = sequence[i]\n",
        "                self.samples.append((input_seq, target))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target = self.samples[idx]\n",
        "        return input_seq, target\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for batching sequences of different lengths.\n",
        "    Pad to maximum length in batch.\n",
        "    \"\"\"\n",
        "    input_seqs, targets = zip(*batch)\n",
        "    \n",
        "    max_len = max(len(seq) for seq in input_seqs)\n",
        "    \n",
        "    padded_seqs = []\n",
        "    attention_masks = []\n",
        "    \n",
        "    for seq in input_seqs:\n",
        "        padding_len = max_len - len(seq)\n",
        "        padded_seq = [0] * padding_len + list(seq)  # left padding\n",
        "        mask = [0] * padding_len + [1] * len(seq)\n",
        "        padded_seqs.append(padded_seq)\n",
        "        attention_masks.append(mask)\n",
        "    \n",
        "    return (\n",
        "        torch.tensor(padded_seqs, dtype=torch.long),\n",
        "        torch.tensor(attention_masks, dtype=torch.float),\n",
        "        torch.tensor(targets, dtype=torch.long)\n",
        "    )\n",
        "\n",
        "\n",
        "def split_sequences(\n",
        "    user_sequences: Dict[int, List[int]],\n",
        "    val_ratio: float = 0.1,\n",
        "    test_ratio: float = 0.1\n",
        ") -> Tuple[Dict[int, List[int]], Dict[int, List[int]], Dict[int, List[int]]]:\n",
        "    \"\"\"\n",
        "    Split sequences into train/val/test.\n",
        "    For each user, use leave-last-out strategy:\n",
        "    - Last item -> test\n",
        "    - Second to last -> val\n",
        "    - Rest -> train\n",
        "    \"\"\"\n",
        "    train_seqs = {}\n",
        "    val_seqs = {}\n",
        "    test_seqs = {}\n",
        "    \n",
        "    for user_idx, seq in user_sequences.items():\n",
        "        if len(seq) >= 3:\n",
        "            train_seqs[user_idx] = seq[:-2]\n",
        "            val_seqs[user_idx] = seq[:-1]  # include full history for validation\n",
        "            test_seqs[user_idx] = seq  # include full history for test\n",
        "    \n",
        "    return train_seqs, val_seqs, test_seqs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## transformer decoder model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model: int, max_len: int = 1024, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"Args: x: Tensor of shape [batch_size, seq_len, d_model]\"\"\"\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class TransformerRecommender(nn.Module):\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        num_items: int,\n",
        "        d_model: int = 64,\n",
        "        nhead: int = 2,\n",
        "        num_layers: int = 2,\n",
        "        dim_feedforward: int = 256,\n",
        "        dropout: float = 0.1,\n",
        "        max_seq_len: int = 50\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_items = num_items\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # item embedding (0 padding)\n",
        "        self.item_embedding = nn.Embedding(num_items + 1, d_model, padding_idx=0)\n",
        "        \n",
        "        # positional encoding (use larger max_len to flex)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=1024, dropout=dropout)\n",
        "        \n",
        "        # layers\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "        \n",
        "        # output layer\n",
        "        self.output_layer = nn.Linear(d_model, num_items + 1)\n",
        "        \n",
        "        self._init_weights()\n",
        "    \n",
        "    def _init_weights(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    \n",
        "    def generate_causal_mask(self, seq_len: int) -> torch.Tensor:\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "        return mask\n",
        "    \n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        x = self.item_embedding(input_ids)\n",
        "        x = self.pos_encoding(x)\n",
        "        \n",
        "        causal_mask = self.generate_causal_mask(seq_len).to(input_ids.device)\n",
        "        \n",
        "        padding_mask = (attention_mask == 0) if attention_mask is not None else None\n",
        "        \n",
        "        memory = torch.zeros(batch_size, 1, self.d_model, device=input_ids.device)\n",
        "        output = self.transformer_decoder(\n",
        "            x, memory, tgt_mask=causal_mask, tgt_key_padding_mask=padding_mask\n",
        "        )\n",
        "        \n",
        "        logits = self.output_layer(output)\n",
        "        return logits\n",
        "    \n",
        "    def predict_next(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None, top_k: int = 10\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        logits = self.forward(input_ids, attention_mask)\n",
        "        \n",
        "        last_logits = logits[:, -1, :]\n",
        "        \n",
        "        last_logits[:, 0] = float('-inf')\n",
        "        \n",
        "        for i in range(input_ids.shape[0]):\n",
        "            seen_items = input_ids[i][input_ids[i] != 0].tolist()\n",
        "            last_logits[i, seen_items] = float('-inf')\n",
        "        \n",
        "        # top-k\n",
        "        probs = F.softmax(last_logits, dim=-1)\n",
        "        top_k_scores, top_k_items = torch.topk(probs, top_k, dim=-1)\n",
        "        \n",
        "        return top_k_items, top_k_scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_model(\n",
        "    model: nn.Module,\n",
        "    movie2idx: Dict[int, int],\n",
        "    idx2movie_title: Dict[int, str],\n",
        "    filepath: str = \"model_checkpoint.pt\"\n",
        "):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_config': {\n",
        "            'num_items': model.num_items,\n",
        "            'd_model': model.d_model,\n",
        "            'nhead': model.transformer_decoder.layers[0].self_attn.num_heads,\n",
        "            'num_layers': len(model.transformer_decoder.layers),\n",
        "            'dim_feedforward': model.transformer_decoder.layers[0].linear1.out_features,\n",
        "        },\n",
        "        'movie2idx': movie2idx,\n",
        "        'idx2movie_title': idx2movie_title,\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    print(f\"Model saved to '{filepath}'\")\n",
        "\n",
        "\n",
        "def load_model(\n",
        "    filepath: str = \"model_checkpoint.pt\",\n",
        "    device: torch.device = DEVICE\n",
        ") -> Tuple[nn.Module, Dict[int, int], Dict[int, str]]:\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    \n",
        "    # recreate model with saved config\n",
        "    config = checkpoint['model_config']\n",
        "    model = TransformerRecommender(\n",
        "        num_items=config['num_items'],\n",
        "        d_model=config['d_model'],\n",
        "        nhead=config['nhead'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dim_feedforward=config['dim_feedforward'],\n",
        "    )\n",
        "    \n",
        "    # load weights\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    print(f\"Model loaded from '{filepath}'\")\n",
        "    print(f\"  - Number of items: {config['num_items']}\")\n",
        "    print(f\"  - Model dimension: {config['d_model']}\")\n",
        "    print(f\"  - Number of layers: {config['num_layers']}\")\n",
        "    \n",
        "    return model, checkpoint['movie2idx'], checkpoint['idx2movie_title']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## baseline models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PopularityBaseline:    \n",
        "    def __init__(self, user_sequences: Dict[int, List[int]]):\n",
        "        all_items = []\n",
        "        for seq in user_sequences.values():\n",
        "            all_items.extend(seq)\n",
        "        \n",
        "        self.item_counts = Counter(all_items)\n",
        "        self.popular_items = [item for item, _ in self.item_counts.most_common()]\n",
        "    \n",
        "    def predict(self, input_seq: List[int], top_k: int = 10) -> List[int]:\n",
        "        seen = set(input_seq)\n",
        "        predictions = [item for item in self.popular_items if item not in seen]\n",
        "        return predictions[:top_k]\n",
        "\n",
        "\n",
        "class MarkovBaseline:    \n",
        "    def __init__(self, user_sequences: Dict[int, List[int]]):\n",
        "        self.transitions = defaultdict(Counter)\n",
        "        \n",
        "        for seq in user_sequences.values():\n",
        "            for i in range(len(seq) - 1):\n",
        "                self.transitions[seq[i]][seq[i + 1]] += 1\n",
        "        \n",
        "        # popularity as fallback\n",
        "        all_items = []\n",
        "        for seq in user_sequences.values():\n",
        "            all_items.extend(seq)\n",
        "        self.popular_items = [item for item, _ in Counter(all_items).most_common()]\n",
        "    \n",
        "    def predict(self, input_seq: List[int], top_k: int = 10) -> List[int]:\n",
        "        \"\"\"Predict based on last movie in sequence.\"\"\"\n",
        "        if not input_seq:\n",
        "            return self.popular_items[:top_k]\n",
        "        \n",
        "        last_item = input_seq[-1]\n",
        "        seen = set(input_seq)\n",
        "        \n",
        "        if last_item in self.transitions:\n",
        "            candidates = self.transitions[last_item].most_common()\n",
        "            predictions = [item for item, _ in candidates if item not in seen]\n",
        "            if len(predictions) >= top_k:\n",
        "                return predictions[:top_k]\n",
        "            # Fill with popular items\n",
        "            for item in self.popular_items:\n",
        "                if item not in seen and item not in predictions:\n",
        "                    predictions.append(item)\n",
        "                    if len(predictions) >= top_k:\n",
        "                        break\n",
        "            return predictions[:top_k]\n",
        "        \n",
        "        # fallback to popularity\n",
        "        return [item for item in self.popular_items if item not in seen][:top_k]\n",
        "\n",
        "\n",
        "class RandomBaseline:\n",
        "    \"\"\"Random baseline.\"\"\"\n",
        "    \n",
        "    def __init__(self, num_items: int):\n",
        "        self.num_items = num_items\n",
        "    \n",
        "    def predict(self, input_seq: List[int], top_k: int = 10) -> List[int]:\n",
        "        \"\"\"Random movie selection (excluding already watched).\"\"\"\n",
        "        seen = set(input_seq)\n",
        "        available = [i for i in range(1, self.num_items + 1) if i not in seen]\n",
        "        return random.sample(available, min(top_k, len(available)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hit_rate_at_k(predictions: List[int], target: int, k: int = 10) -> float:\n",
        "    return 1.0 if target in predictions[:k] else 0.0\n",
        "\n",
        "\n",
        "def mrr(predictions: List[int], target: int) -> float:\n",
        "    try:\n",
        "        rank = predictions.index(target) + 1\n",
        "        return 1.0 / rank\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def ndcg_at_k(predictions: List[int], target: int, k: int = 10) -> float:\n",
        "    try:\n",
        "        rank = predictions[:k].index(target) + 1\n",
        "        dcg = 1.0 / math.log2(rank + 1)\n",
        "        idcg = 1.0\n",
        "        return dcg / idcg\n",
        "    except ValueError:\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    test_sequences: Dict[int, List[int]],\n",
        "    top_k: int = 10,\n",
        "    max_seq_len: int = 50,\n",
        "    device: torch.device = DEVICE\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Evaluate model on test data.\"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    hr_scores, mrr_scores, ndcg_scores = [], [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for user_idx, seq in tqdm(test_sequences.items(), desc=\"Evaluating model\"):\n",
        "            if len(seq) < 2:\n",
        "                continue\n",
        "            \n",
        "            input_seq = seq[:-1]\n",
        "            target = seq[-1]\n",
        "            \n",
        "            # truncate to max_seq_len\n",
        "            if len(input_seq) > max_seq_len:\n",
        "                input_seq = input_seq[-max_seq_len:]\n",
        "            \n",
        "            input_tensor = torch.tensor([input_seq], dtype=torch.long, device=device)\n",
        "            attention_mask = torch.ones_like(input_tensor, dtype=torch.float)\n",
        "            \n",
        "            top_k_items, _ = model.predict_next(input_tensor, attention_mask, top_k=top_k)\n",
        "            predictions = top_k_items[0].cpu().tolist()\n",
        "            \n",
        "            hr_scores.append(hit_rate_at_k(predictions, target, top_k))\n",
        "            mrr_scores.append(mrr(predictions, target))\n",
        "            ndcg_scores.append(ndcg_at_k(predictions, target, top_k))\n",
        "    \n",
        "    return {\n",
        "        f'HR@{top_k}': np.mean(hr_scores),\n",
        "        'MRR': np.mean(mrr_scores),\n",
        "        f'NDCG@{top_k}': np.mean(ndcg_scores)\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_baseline(\n",
        "    baseline,\n",
        "    test_sequences: Dict[int, List[int]],\n",
        "    top_k: int = 10\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Evaluate baseline model on test data.\"\"\"\n",
        "    hr_scores, mrr_scores, ndcg_scores = [], [], []\n",
        "    \n",
        "    for user_idx, seq in tqdm(test_sequences.items(), desc=\"Evaluating baseline\"):\n",
        "        if len(seq) < 2:\n",
        "            continue\n",
        "        \n",
        "        input_seq = seq[:-1]\n",
        "        target = seq[-1]\n",
        "        \n",
        "        predictions = baseline.predict(input_seq, top_k=top_k)\n",
        "        \n",
        "        hr_scores.append(hit_rate_at_k(predictions, target, top_k))\n",
        "        mrr_scores.append(mrr(predictions, target))\n",
        "        ndcg_scores.append(ndcg_at_k(predictions, target, top_k))\n",
        "    \n",
        "    return {\n",
        "        f'HR@{top_k}': np.mean(hr_scores),\n",
        "        'MRR': np.mean(mrr_scores),\n",
        "        f'NDCG@{top_k}': np.mean(ndcg_scores)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    val_sequences: Dict[int, List[int]],\n",
        "    num_epochs: int = 10,\n",
        "    lr: float = 0.001,\n",
        "    device: torch.device = DEVICE,\n",
        "    patience: int = 3,\n",
        "    max_seq_len: int = 50\n",
        ") -> nn.Module:\n",
        "    \"\"\"Train model with early stopping.\"\"\"\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    \n",
        "    best_val_hr = 0\n",
        "    patience_counter = 0\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "        \n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        for batch in pbar:\n",
        "            input_ids, attention_mask, targets = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            targets = targets.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            logits = model(input_ids, attention_mask)\n",
        "            last_logits = logits[:, -1, :]\n",
        "            \n",
        "            loss = criterion(last_logits, targets)\n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "            pbar.set_postfix({'loss': total_loss / num_batches})\n",
        "        \n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch + 1}: Train Loss = {avg_loss:.4f}\")\n",
        "        \n",
        "        # validation\n",
        "        val_metrics = evaluate_model(model, val_sequences, top_k=10, max_seq_len=max_seq_len, device=device)\n",
        "        print(f\"Validation: HR@10 = {val_metrics['HR@10']:.4f}, MRR = {val_metrics['MRR']:.4f}, NDCG@10 = {val_metrics['NDCG@10']:.4f}\")\n",
        "        \n",
        "        # early stopping\n",
        "        if val_metrics['HR@10'] > best_val_hr:\n",
        "            best_val_hr = val_metrics['HR@10']\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pt')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                break\n",
        "    \n",
        "    # load best model\n",
        "    model.load_state_dict(torch.load('best_model.pt'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_recommendations(\n",
        "    model: nn.Module,\n",
        "    test_sequences: Dict[int, List[int]],\n",
        "    idx2movie_title: Dict[int, str],\n",
        "    num_examples: int = 5,\n",
        "    top_k: int = 5,\n",
        "    max_seq_len: int = 50,\n",
        "    device: torch.device = DEVICE\n",
        "):\n",
        "    model.eval()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"RECOMMENDATION DEMONSTRATION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    users = list(test_sequences.keys())\n",
        "    sample_users = random.sample(users, min(num_examples, len(users)))\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for user_idx in sample_users:\n",
        "            seq = test_sequences[user_idx]\n",
        "            if len(seq) < 3:\n",
        "                continue\n",
        "            \n",
        "            input_seq = seq[:-1]\n",
        "            target = seq[-1]\n",
        "            \n",
        "            print(f\"\\n{'─' * 60}\")\n",
        "            print(f\"User {user_idx}\")\n",
        "            print(f\"Watch history ({len(input_seq)} movies):\")\n",
        "            for i, movie_idx in enumerate(input_seq[-5:], 1):\n",
        "                title = idx2movie_title.get(movie_idx, f\"Movie_{movie_idx}\")\n",
        "                print(f\"  {i}. {title}\")\n",
        "            \n",
        "            if len(input_seq) > 5:\n",
        "                print(f\"  ... and {len(input_seq) - 5} more movies\")\n",
        "            \n",
        "            if len(input_seq) > max_seq_len:\n",
        "                input_seq = input_seq[-max_seq_len:]\n",
        "            \n",
        "            input_tensor = torch.tensor([input_seq], dtype=torch.long, device=device)\n",
        "            attention_mask = torch.ones_like(input_tensor, dtype=torch.float)\n",
        "            top_k_items, top_k_scores = model.predict_next(input_tensor, attention_mask, top_k=top_k)\n",
        "            \n",
        "            predictions = top_k_items[0].cpu().tolist()\n",
        "            scores = top_k_scores[0].cpu().tolist()\n",
        "            \n",
        "            print(f\"\\nModel recommendations:\")\n",
        "            for i, (movie_idx, score) in enumerate(zip(predictions, scores), 1):\n",
        "                title = idx2movie_title.get(movie_idx, f\"Movie_{movie_idx}\")\n",
        "                hit_marker = \"✓\" if movie_idx == target else \" \"\n",
        "                print(f\"  {i}. {hit_marker} {title} (score: {score:.4f})\")\n",
        "            \n",
        "            target_title = idx2movie_title.get(target, f\"Movie_{target}\")\n",
        "            print(f\"\\nActual next movie: {target_title}\")\n",
        "            if target in predictions:\n",
        "                rank = predictions.index(target) + 1\n",
        "                print(f\"→ Position in recommendations: {rank}\")\n",
        "            else:\n",
        "                print(f\"→ Not in top-{top_k}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## prepare data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Loaded 100004 ratings\n",
            "Loaded 45463 movies\n",
            "After user filtering: 100004 ratings\n",
            "After movie filtering: 90072 ratings\n",
            "Unique users: 671\n",
            "Unique movies: 3496\n",
            "Creating sequences (min_rating=3.5)...\n",
            "Positive interactions: 56892\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating sequences: 100%|██████████| 56892/56892 [00:02<00:00, 22979.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Users with sequences >= 3: 669\n",
            "Sequence length statistics:\n",
            "  Min: 3, Max: 913, Mean: 85.04\n",
            "\n",
            "Dataset sizes:\n",
            "  Train: 669 users\n",
            "  Val: 669 users\n",
            "  Test: 669 users\n",
            "\n",
            "Training samples: 54882\n"
          ]
        }
      ],
      "source": [
        "ratings, movies, user2idx, movie2idx, idx2movie_title = load_and_preprocess_data()\n",
        "num_items = len(movie2idx)\n",
        "\n",
        "user_sequences = create_user_sequences(ratings, user2idx, movie2idx)\n",
        "\n",
        "train_seqs, val_seqs, test_seqs = split_sequences(user_sequences)\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train: {len(train_seqs)} users\")\n",
        "print(f\"  Val: {len(val_seqs)} users\")\n",
        "print(f\"  Test: {len(test_seqs)} users\")\n",
        "\n",
        "train_dataset = MovieSequenceDataset(train_seqs, max_seq_len=50, num_items=num_items)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=0\n",
        ")\n",
        "print(f\"\\nTraining samples: {len(train_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Architecture:\n",
            "  Number of items: 3496\n",
            "  Embedding dimension: 64\n",
            "  Number of heads: 2\n",
            "  Number of layers: 2\n",
            "  FFN dimension: 256\n",
            "  Total parameters: 584,617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15:   0%|          | 0/858 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|██████████| 858/858 [03:33<00:00,  4.03it/s, loss=7.55]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss = 7.5479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating model: 100%|██████████| 669/669 [00:03<00:00, 186.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation: HR@10 = 0.0224, MRR = 0.0039, NDCG@10 = 0.0081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/15: 100%|██████████| 858/858 [04:35<00:00,  3.12it/s, loss=7.3] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss = 7.2976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating model: 100%|██████████| 669/669 [00:07<00:00, 92.27it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation: HR@10 = 0.0314, MRR = 0.0173, NDCG@10 = 0.0205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/15: 100%|██████████| 858/858 [05:30<00:00,  2.59it/s, loss=7.2] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss = 7.2004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating model: 100%|██████████| 669/669 [00:06<00:00, 102.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation: HR@10 = 0.0314, MRR = 0.0155, NDCG@10 = 0.0192\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/15:  43%|████▎     | 371/858 [01:28<01:44,  4.66it/s, loss=7.07]"
          ]
        }
      ],
      "source": [
        "\n",
        "model = TransformerRecommender(\n",
        "    num_items=num_items,\n",
        "    d_model=64,\n",
        "    nhead=2,\n",
        "    num_layers=2,\n",
        "    dim_feedforward=256,\n",
        "    dropout=0.1,\n",
        "    max_seq_len=50\n",
        ")\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(f\"  Number of items: {num_items}\")\n",
        "print(f\"  Embedding dimension: 64\")\n",
        "print(f\"  Number of heads: 2\")\n",
        "print(f\"  Number of layers: 2\")\n",
        "print(f\"  FFN dimension: 256\")\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"  Total parameters: {total_params:,}\")\n",
        "\n",
        "MAX_SEQ_LEN = 50\n",
        "\n",
        "model = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_seqs,\n",
        "    num_epochs=15,\n",
        "    lr=0.001,\n",
        "    device=DEVICE,\n",
        "    patience=3,\n",
        "    max_seq_len=MAX_SEQ_LEN\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "save_model(model, movie2idx, idx2movie_title, \"model_checkpoint.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## init baseline models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "popularity_baseline = PopularityBaseline(train_seqs)\n",
        "markov_baseline = MarkovBaseline(train_seqs)\n",
        "random_baseline = RandomBaseline(num_items)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## evaulate all\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all models\n",
        "results = {}\n",
        "\n",
        "print(\"Evaluating Transformer model...\")\n",
        "transformer_metrics = evaluate_model(model, test_seqs, top_k=10, max_seq_len=MAX_SEQ_LEN, device=DEVICE)\n",
        "results['Transformer'] = transformer_metrics\n",
        "\n",
        "print(\"\\nEvaluating Popularity Baseline...\")\n",
        "popularity_metrics = evaluate_baseline(popularity_baseline, test_seqs, top_k=10)\n",
        "results['Popularity'] = popularity_metrics\n",
        "\n",
        "print(\"\\nEvaluating Markov Baseline...\")\n",
        "markov_metrics = evaluate_baseline(markov_baseline, test_seqs, top_k=10)\n",
        "results['Markov'] = markov_metrics\n",
        "\n",
        "print(\"\\nEvaluating Random Baseline...\")\n",
        "random_metrics = evaluate_baseline(random_baseline, test_seqs, top_k=10)\n",
        "results['Random'] = random_metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n{'Model':<20} {'HR@10':<12} {'MRR':<12} {'NDCG@10':<12}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"{model_name:<20} {metrics['HR@10']:<12.4f} {metrics['MRR']:<12.4f} {metrics['NDCG@10']:<12.4f}\")\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"\\n\")\n",
        "display(results_df.round(4))\n",
        "\n",
        "results_df.to_csv('results.csv')\n",
        "print(\"\\nResults saved to 'results.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## demostration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demonstrate_recommendations(\n",
        "    model, \n",
        "    test_seqs, \n",
        "    idx2movie_title, \n",
        "    num_examples=5, \n",
        "    top_k=5, \n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    device=DEVICE\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaded_model, loaded_movie2idx, loaded_idx2movie_title = load_model(\"model_checkpoint.pt\")\n",
        "\n",
        "test_metrics = evaluate_model(loaded_model, test_seqs, top_k=10, max_seq_len=MAX_SEQ_LEN, device=DEVICE)\n",
        "print(f\"Loaded model - HR@10: {test_metrics['HR@10']:.4f}, MRR: {test_metrics['MRR']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
